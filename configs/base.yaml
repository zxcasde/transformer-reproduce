# config.yaml - 完整训练配置示例
project: "text_classification"
version: "1.0"

# 硬件配置
device:
  gpus: "0,1"
  use_cuda: true

# 路径配置
paths:
  data_dir: "./data"
  output_dir: "./output"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"

# 模型配置
model:
  type: "bert"
  pretrained: "bert-base-uncased"
  num_classes: 2
  dropout: 0.1

# 训练配置
training:
  seed: 42
  batch_size: 32
  micro_batch_size: 8  # 梯度累积
  gradient_accumulation_steps: 4
  epochs: 50
  max_steps: 100000
  
  # 优化器
  optimizer:
    name: "adamw"
    learning_rate: 2e-5
    weight_decay: 0.01
    beta1: 0.9
    beta2: 0.999
    eps: 1e-8
    
  # 学习率调度
  lr_scheduler:
    name: "linear_warmup"
    warmup_steps: 1000
    total_steps: 10000
    
  # 梯度裁剪
  max_grad_norm: 1.0

# 评估配置
evaluation:
  eval_batch_size: 64
  eval_steps: 500
  save_steps: 1000
  eval_strategy: "steps"

# 硬件配置
hardware:
  device: "cuda"
  fp16: true
  dataloader_workers: 4